# Prompt-Injection-Attacks-on-AI-Systems
This repository explores the security vulnerabilities of large language models (LLMs) to prompt injection attacks. It includes a research paper, benchmarks, attack/defense taxonomies, and illustrations of both direct and indirect prompt injections. Ideal for researchers, developers, and security practitioners working on LLM safety.
